{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Our Method\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'pandas._libs.tslibs.conversion._TSObject' has no attribute '__reduce_cython__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a4b8a2f82adc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[1;31m#trajectory2Vec()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;31m#print(\"FINISHHHHHHHH\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[0mvecClusterAnalysis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-a4b8a2f82adc>\u001b[0m in \u001b[0;36mvecClusterAnalysis\u001b[1;34m()\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'Our Method'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m     \u001b[0mtrVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m     \u001b[0mtrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./simulated_data/sim_traj_vec_normal_humanMvtMainGauche_reverse'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m     \u001b[0minte\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\_api\\v1\\compat\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_print_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\_api\\v1\\compat\\v1\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[0m_current_module\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m   _current_module.__path__ = (\n\u001b[0;32m    651\u001b[0m       [_module_util.get_parent_dir(estimator)] + _current_module.__path__)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\estimator_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msession_run_hook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel_to_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode_keys\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\inputs.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# pylint: disable=unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy_input_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas_input_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\numpy_io.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueues\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeeding_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator_export\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_estimator\\python\\estimator\\inputs\\queues\\feeding_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m   \u001b[1;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m   \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m   \u001b[0mHAS_PANDAS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhashtable\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_hashtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtslib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_tslib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# hack but overkill to use re\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m from .tslibs import (\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mNaTType\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\tslibs\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# flake8: noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlocalize_pydatetime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize_date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnattype\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNaTType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miNaT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_null_datetimelike\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mnp_datetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOutOfBoundsDatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\tslibs\\conversion.pyx\u001b[0m in \u001b[0;36minit pandas._libs.tslibs.conversion\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'pandas._libs.tslibs.conversion._TSObject' has no attribute '__reduce_cython__'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle as cPickle\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import *\n",
    "from sklearn import preprocessing\n",
    "\n",
    "tf.reset_default_graph()\n",
    "random.seed(2016)\n",
    "sampleNum = 5#10\n",
    "\n",
    "def completeTrajectories():\n",
    "    N = 3\n",
    "    path = './X.npy'\n",
    "    x = np.load(path)\n",
    "   # print(\"===========\")\n",
    "   # print(x.shape[0])\n",
    "   # print(x)\n",
    "   # print(\"===========\")\n",
    "    mainGauchesTraj = []\n",
    "    simTrjComps = []\n",
    "    simTrjss = []\n",
    "    simSeq = []\n",
    "    numJoints=20\n",
    "    #dim=3\n",
    "    allX = []\n",
    "    allY = []\n",
    "    allZ = []\n",
    "    joints = []\n",
    "    for seq in x[:100]:\n",
    "        #print(seq)\n",
    "        gaucheOnly = []\n",
    "        for i in range(0, seq.shape[0]):\n",
    "           \n",
    "            #print(seq[i])\n",
    "            joints = []\n",
    "            joints = [seq[i][n:n+N] for n in range(0, len(seq[i]), N)]\n",
    "            #print(\"Main Gauche\")\n",
    "            #print(joints[10])\n",
    "            gaucheOnly.append(joints[10])\n",
    "            #joints.append(subList)\n",
    "            #simTrjss.append(joints)\n",
    "            #simSeq.append(simTrjss)\n",
    "        mainGauchesTraj.append(gaucheOnly)\n",
    "    \n",
    "    print(mainGauchesTraj)\n",
    "    \n",
    "    for simTrjs in mainGauchesTraj:\n",
    "        trjsCom = []\n",
    "        time = 0\n",
    "        for i in range(0,len(simTrjs)):\n",
    "            rec = []\n",
    "\n",
    "            if i==0:\n",
    "                # time, locationC, speedC, rotC\n",
    "                rec = [time,0,0]#,0]\n",
    "            else:\n",
    "                time = time + 5\n",
    "                locC = math.sqrt((simTrjs[i][0]-simTrjs[i-1][0])**2+(simTrjs[i][1]-simTrjs[i-1][1])**2+(simTrjs[i][2]-simTrjs[i-1][2])**2)\n",
    "                rec.append(time)\n",
    "                rec.append(locC)\n",
    "                #rec.append(locC/(simTrjs[i][0]-simTrjs[i-1][0]))\n",
    "                rec.append(locC/5)\n",
    "                #rec.append(math.atan((simTrjs[i][2]-simTrjs[i-1][2])/ (simTrjs[i][1]-simTrjs[i-1][1])))\n",
    "            trjsCom.append(rec)\n",
    "        simTrjComps.append(trjsCom)\n",
    "    print(\"DONE=========\")\n",
    "    print(simTrjComps)\n",
    "    \n",
    "    cPickle.dump(simTrjComps,open('./simulated_data/sim_traj_humanMvtMainGauche_complete','wb'))\n",
    "    return simTrjComps\n",
    "\n",
    "def computeFeas():\n",
    "    simTrjCompss = cPickle.load(open('./simulated_data/sim_traj_humanMvtMainGauche_complete','rb'))\n",
    "    simTrjFeas = []\n",
    "    for simTrjComps in simTrjCompss:\n",
    "        trjsComfea = []\n",
    "        for i in range(0,len(simTrjComps)):\n",
    "            rec = []\n",
    "            if i==0:\n",
    "                # time, locationC, speedC, rotC\n",
    "                rec = [0,0,0]#,0]\n",
    "            else:\n",
    "                locC = simTrjComps[i][1]\n",
    "                locCrate = locC/(simTrjComps[i][0]-simTrjComps[i-1][0])\n",
    "                rec.append(simTrjComps[i][0])\n",
    "                rec.append(locCrate)\n",
    "                if locCrate<3:\n",
    "                    rec.append(0)\n",
    "                    #rec.append(0)\n",
    "                else:\n",
    "                    rec.append(simTrjComps[i][2]-simTrjComps[i-1][2])\n",
    "                    #rec.append(simTrjComps[i][3]-simTrjComps[i-1][3])\n",
    "            trjsComfea.append(rec)\n",
    "        simTrjFeas.append(trjsComfea)\n",
    "    cPickle.dump(simTrjFeas, open('./simulated_data/sim_traj_humanMvtMainGauche_feas', 'wb'))\n",
    "    #print(simTrjFeas)\n",
    "    return simTrjFeas\n",
    "\n",
    "def rolling_window(sample, windowsize = 600, offset = 300):\n",
    "    #print(\"ROLLING WINDOW\")\n",
    "    timeLength = sample[len(sample)-1][0]\n",
    "    windowLength = int (timeLength/offset)+1\n",
    "    windows = []\n",
    "    for i in range(0,windowLength):\n",
    "        windows.append([])\n",
    "\n",
    "    for record in sample:\n",
    "        time = record[0]\n",
    "        for i in range(0,windowLength):\n",
    "            if (time>(i*offset)) & (time<(i*offset+windowsize)):\n",
    "                windows[i].append(record)\n",
    "                \n",
    "   # print(windows)\n",
    "    return windows\n",
    "    # pass\n",
    "\n",
    "def behavior_ext(windows):\n",
    "    #print(\"BEHAVIOR EXTTTTTTTTTTT\")\n",
    "    \n",
    "    behavior_sequence = []\n",
    "    for window in windows:\n",
    "        behaviorFeature = []\n",
    "        records = np.array(window)\n",
    "        if len(records) != 0:\n",
    "            # print np.shape(records)\n",
    "            pd = pandas.DataFrame(records)\n",
    "            pdd =  pd.describe()\n",
    "            #print(pdd)\n",
    "            # print pdd[1][0]\n",
    "            # for ii in range(1,4):\n",
    "            #     for jj in range(1,8):\n",
    "            #         behaviorFeature.append(pdd[ii][jj])\n",
    "            # behaviorFeature.append(pdd[0][1])\n",
    "            behaviorFeature.append(pdd[1][1])\n",
    "            behaviorFeature.append(pdd[2][1])\n",
    "            #behaviorFeature.append(pdd[3][1])\n",
    "            # behaviorFeature.append(pdd[0][2])\n",
    "            # behaviorFeature.append(pdd[1][2])\n",
    "            # behaviorFeature.append(pdd[2][2])\n",
    "            # behaviorFeature.append(pdd[3][2])\n",
    "            # behaviorFeature.append(pdd[0][3])\n",
    "            behaviorFeature.append(pdd[1][3])\n",
    "            behaviorFeature.append(pdd[2][3])\n",
    "            #behaviorFeature.append(pdd[3][3])\n",
    "            # behaviorFeature.append(pdd[0][4])\n",
    "            behaviorFeature.append(pdd[1][4])\n",
    "            behaviorFeature.append(pdd[2][4])\n",
    "            #behaviorFeature.append(pdd[3][4])\n",
    "            # behaviorFeature.append(pdd[0][5])\n",
    "            behaviorFeature.append(pdd[1][5])\n",
    "            behaviorFeature.append(pdd[2][5])\n",
    "            #behaviorFeature.append(pdd[3][5])\n",
    "            # behaviorFeature.append(pdd[0][6])\n",
    "            behaviorFeature.append(pdd[1][6])\n",
    "            behaviorFeature.append(pdd[2][6])\n",
    "            #behaviorFeature.append(pdd[3][6])\n",
    "            # behaviorFeature.append(pdd[0][7])\n",
    "            behaviorFeature.append(pdd[1][7])\n",
    "            behaviorFeature.append(pdd[2][7])\n",
    "            #behaviorFeature.append(pdd[3][7])\n",
    "\n",
    "            behavior_sequence.append(behaviorFeature)\n",
    "    #print(behavior_sequence)\n",
    "    return behavior_sequence\n",
    "\n",
    "def generate_behavior_sequences():\n",
    "    f = open('./simulated_data/sim_traj_humanMvtMainGauche_feas','rb')\n",
    "    sim_data = cPickle.load(f)\n",
    "    behavior_sequences = []\n",
    "\n",
    "    for sample in sim_data:\n",
    "        #print(sample)\n",
    "        windows = rolling_window(sample)\n",
    "        behavior_sequence = behavior_ext(windows)\n",
    "        print(len(behavior_sequence))\n",
    "        behavior_sequences.append(behavior_sequence)\n",
    "    fout = open('./simulated_data/sim_behavior_humanMvtMainGauche_sequences','wb')\n",
    "    cPickle.dump(behavior_sequences,fout)\n",
    "\n",
    "def generate_normal_behavior_sequence():\n",
    "    print(\"generate_normal_behavior_sequence\")\n",
    "    f = open('./simulated_data/sim_behavior_humanMvtMainGauche_sequences','rb')\n",
    "    behavior_sequences = cPickle.load(f)\n",
    "    print(behavior_sequences)\n",
    "    print(np.shape(behavior_sequences))\n",
    "    behavior_sequences_normal = []\n",
    "    templist = []\n",
    "    for item in behavior_sequences:\n",
    "        for ii in item:\n",
    "            templist.append(ii)\n",
    "        #print (len(item))\n",
    "    #print(len(templist))\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    # print np.shape(behavior_sequence)\n",
    "    templist_normal = min_max_scaler.fit_transform(templist).tolist()\n",
    "    index = 0\n",
    "    for item in behavior_sequences:\n",
    "        behavior_sequence_normal = []\n",
    "        for ii in item:\n",
    "            behavior_sequence_normal.append(templist_normal[index])\n",
    "            index = index + 1\n",
    "        #print(len(behavior_sequence_normal))\n",
    "        behavior_sequences_normal.append(behavior_sequence_normal)\n",
    "    #print(index)\n",
    "    print(\"generate_normal_behavior_sequence222222222222222\")\n",
    "    print(behavior_sequences_normal)\n",
    "    fout = open('./simulated_data/sim_normal_behavior_humanMvtMainGauche_sequences', 'wb')\n",
    "    cPickle.dump(behavior_sequences_normal, fout)\n",
    "\n",
    "\n",
    "    \n",
    "def trajectory2Vec():\n",
    "    def loopf(prev, i):\n",
    "        return prev\n",
    "\n",
    "    # Parameters\n",
    "    learning_rate = 0.0001\n",
    "    training_epochs = 300\n",
    "    display_step = 100\n",
    "\n",
    "    # Network Parameters\n",
    "    # the size of the hidden state for the lstm (notice the lstm uses 2x of this amount so actually lstm will have state of size 2)\n",
    "    size = 100\n",
    "    # 2 different sequences total\n",
    "    batch_size = 1\n",
    "    # the maximum steps for both sequences is 5\n",
    "    max_n_steps = 17\n",
    "    # each element/frame of the sequence has dimension of 3\n",
    "    frame_dim = 12#18\n",
    "\n",
    "    input_length = tf.placeholder(tf.int32)\n",
    "\n",
    "    initializer = tf.random_uniform_initializer(-1, 1)\n",
    "\n",
    "    # the sequences, has n steps of maximum size\n",
    "    # seq_input = tf.placeholder(tf.float32, [batch_size, max_n_steps, frame_dim])\n",
    "    seq_input = tf.placeholder(tf.float32, [max_n_steps, batch_size, frame_dim])\n",
    "    # what timesteps we want to stop at, notice it's different for each batch hence dimension of [batch]\n",
    "\n",
    "    # inputs for rnn needs to be a list, each item/frame being a timestep.\n",
    "    # we need to split our input into each timestep, and reshape it because split keeps dims by default\n",
    "\n",
    "    useful_input = seq_input[0:input_length[0]]\n",
    "    loss_inputs = [tf.reshape(useful_input, [-1])]\n",
    "    encoder_inputs = [item for item in tf.unstack(seq_input)]\n",
    "    # if encoder input is \"X, Y, Z\", then decoder input is \"0, X, Y, Z\". Therefore, the decoder size\n",
    "    # and target size equal encoder size plus 1. For simplicity, here I droped the last one.\n",
    "    decoder_inputs = ([tf.zeros_like(encoder_inputs[0], name=\"GO\")] + encoder_inputs[:-1])\n",
    "    targets = encoder_inputs\n",
    "\n",
    "    # basic LSTM seq2seq model\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(size, state_is_tuple=True, use_peepholes=True)\n",
    "    _, enc_state = tf.contrib.rnn.static_rnn(cell, encoder_inputs, sequence_length=input_length[0], dtype=tf.float32)\n",
    "    cell = tf.contrib.rnn.OutputProjectionWrapper(cell, frame_dim)\n",
    "    dec_outputs, dec_state = tf.contrib.legacy_seq2seq.rnn_decoder(decoder_inputs, enc_state, cell, loop_function=loopf)\n",
    "\n",
    "\n",
    "    # flatten the prediction and target to compute squared error loss\n",
    "    y_true = [tf.reshape(encoder_input, [-1]) for encoder_input in encoder_inputs]\n",
    "    y_pred = [tf.reshape(dec_output, [-1]) for dec_output in dec_outputs]\n",
    "\n",
    "    # Define loss and optimizer, minimize the squared error\n",
    "    loss = 0\n",
    "    for i in range(len(loss_inputs)):\n",
    "        loss += tf.reduce_sum(tf.square(tf.subtract(y_pred[i], y_true[len(loss_inputs) - i - 1])))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        # Training cycle\n",
    "        input_datas = cPickle.load(open('./simulated_data/sim_normal_behavior_humanMvtMainGauche_sequences','rb'))\n",
    "        print(\"INPUT DATAS\")\n",
    "        print(input_datas)\n",
    "        trajectoryVecs = []\n",
    "        j = 0\n",
    "        for input_data in input_datas:\n",
    "            print ('Sample:')\n",
    "            print (j)\n",
    "            input_len = len(input_data)\n",
    "            print (input_len)\n",
    "            defalt = []\n",
    "            for i in range(0, frame_dim):\n",
    "                defalt.append(0)\n",
    "            while len(input_data) < max_n_steps:\n",
    "                input_data.append(defalt)\n",
    "            x = np.array(input_data)\n",
    "            print(\"shape======\")\n",
    "            print (np.shape(x[0]))\n",
    "            print(x)\n",
    "            x = x.reshape((max_n_steps, batch_size, frame_dim))\n",
    "            embedding = None\n",
    "            for epoch in range(training_epochs):\n",
    "                feed = {seq_input: x, input_length: np.array([input_len])}\n",
    "                # Fit training using batch data\n",
    "                _, cost_value, embedding, en_int, de_outs, loss_in = sess.run(\n",
    "                    [optimizer, loss, enc_state, encoder_inputs, dec_outputs, loss_inputs], feed_dict=feed)\n",
    "                # Display logs per epoch step\n",
    "                if epoch % display_step == 0:\n",
    "                    print (\"logits\")\n",
    "                    a = sess.run(y_pred, feed_dict=feed)\n",
    "                    print (\"labels\")\n",
    "                    b = sess.run(y_true, feed_dict=feed)\n",
    "\n",
    "                    print(\"Epoch:\", '%04d' % (epoch + 1), \"cost=\", \"{:.9f}\".format(cost_value))\n",
    "            trajectoryVecs.append(embedding)\n",
    "            print(\"Optimization Finished!\")\n",
    "            j = j + 1\n",
    "        fout = open('./simulated_data/sim_traj_vec_normal_humanMvtMainGauche_reverse', 'wb')\n",
    "        cPickle.dump(trajectoryVecs, fout)\n",
    "\n",
    "def vecClusterAnalysis():\n",
    "    print ('---------------------------------')\n",
    "    print ('Our Method')\n",
    "    trVecs = []\n",
    "    trs = cPickle.load(open('./simulated_data/sim_traj_vec_normal_humanMvtMainGauche_reverse','rb'))\n",
    "    print(trs)\n",
    "    inte = []\n",
    "    for tr in trs:\n",
    "        print(\"-----------\")\n",
    "        print(tr)\n",
    "        trVecs.append(tr[0][0])\n",
    "        \n",
    "    print(\"TRRR\")\n",
    "    print(tr)\n",
    "    km = KMeans(n_clusters=20, random_state=2016)\n",
    "    clusters = km.fit(trVecs).labels_.tolist()\n",
    "    print(\"clusters\")\n",
    "    print(clusters)\n",
    "    print ('---------------------------------')\n",
    "    all = 0.\n",
    "    print(\"###\")\n",
    "    print(clusters[:sampleNum])\n",
    "    item = set(clusters[:sampleNum])\n",
    "    print(\"itemmmm\")\n",
    "    print(item)\n",
    "    l = []\n",
    "    \n",
    "    \n",
    "    for i in item:\n",
    "        l.append([i,clusters[:sampleNum].count(i)])\n",
    "        print('Straight dans for:  '+ str(l))\n",
    "        \n",
    "    print ('Straight:  '+ str(l))\n",
    "    m = max([te[1] for te in l])\n",
    "    print(\"MAXXX\",m)\n",
    "    all = all + m\n",
    "    print (float(m)/sampleNum)\n",
    "\n",
    "\n",
    "    m = 0.\n",
    "    item = set(clusters[sampleNum:sampleNum*2])\n",
    "    l = []\n",
    "    for i in item:\n",
    "        l.append([i,clusters[sampleNum:sampleNum*2].count(i)])\n",
    "    print ('Circling:  '+ str(l))\n",
    "    m = max([te[1] for te in l])\n",
    "    all = all + m\n",
    "    print (float(m)/sampleNum)\n",
    "\n",
    "    m = 0.\n",
    "    item = set(clusters[sampleNum*2:sampleNum*3])\n",
    "    l = []\n",
    "    for i in item:\n",
    "        l.append([i,clusters[sampleNum*2:sampleNum*3].count(i)])\n",
    "    m = max([te[1] for te in l])\n",
    "    print ('bending:   '+ str(l))\n",
    "    all = all + m\n",
    "    print (float(m)/sampleNum)\n",
    "    print ('overall')\n",
    "    print (all/(sampleNum*3))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #completeTrajectories()\n",
    "    #computeFeas()\n",
    "    #generate_behavior_sequences()\n",
    "    #generate_normal_behavior_sequence()\n",
    "    #trajectory2Vec()\n",
    "    #print(\"FINISHHHHHHHH\")\n",
    "    vecClusterAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-1.13",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
